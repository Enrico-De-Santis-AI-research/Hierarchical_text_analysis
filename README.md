# ğŸ“Š Linguistic Feature Analysis using Maximum Entropy & Neural Network Dependencies

## **ğŸš€ Overview**
This project focuses on **analyzing linguistic structures** using **Maximum Entropy models** with **Lagrange multipliers** and integrating **neural network-based feature dependencies** to regularize constraints.

### **Key Features:**
- **Synthetic dataset generation** for linguistic indices.
- **Feature hierarchy analysis** across lexical, syntactic, semantic, and stylistic dimensions.
- **Maximum Entropy training** to estimate Lagrange multipliers using different constraints (mean, variance, skewness, kurtosis).
- **Neural Network regularization** to enforce **nonlinear dependencies** between features.
- **Statistical validation** (ANOVA, t-tests, effect size) to evaluate constraint significance.
- **Visualization and analysis** using heatmaps, boxplots, and correlation matrices.

---

## **ğŸ“‚ Project Structure**
```
ğŸ“¦ Linguistic-Entropy-Model
â”‚
â”œâ”€â”€ ğŸ“‚ Data                           # ğŸ”¹ Contains synthetic & processed datasets
â”‚   â”œâ”€â”€ fake_data_nextLevel.csv
â”‚   â”œâ”€â”€ fake_data_nextLevel_normalized.csv
â”‚   â”œâ”€â”€ fake_data_test_normalized.csv
â”‚
â”œâ”€â”€ ğŸ“‚ Models                         # ğŸ”¹ Trained Neural Network models for feature dependencies
â”‚   â”œâ”€â”€ dependency_nn_best.pth
â”‚
â”œâ”€â”€ ğŸ“‚ Results                        # ğŸ”¹ Stores results from Maximum Entropy training & grid search
â”‚   â”œâ”€â”€ Grid_Search_mean/
â”‚   â”œâ”€â”€ Trained_MaxEnt_mean/
â”‚   â”œâ”€â”€ Statistical_Plots/
â”‚   â”œâ”€â”€ NN_Feature_Dependencies.csv
â”‚   â”œâ”€â”€ Lagrange_Multipliers.csv
â”‚
â”œâ”€â”€ ğŸ“‚ Scripts                        # ğŸ”¹ Core scripts for training, testing, and visualization
â”‚   â”œâ”€â”€ generate_fake_data_nextLevel.py
â”‚   â”œâ”€â”€ generate_fake_test_data.py
â”‚   â”œâ”€â”€ train_dependency_nn.py
â”‚   â”œâ”€â”€ train_best_maxent_train_test_split.py
â”‚   â”œâ”€â”€ maxent_grid_search.py
â”‚   â”œâ”€â”€ statistical_analysis.py
â”‚   â”œâ”€â”€ interpret_results.py
â”‚
â”œâ”€â”€ README.md                         # ğŸ”¹ Project documentation (this file)
â””â”€â”€ requirements.txt                   # ğŸ”¹ Required Python dependencies
```

---

## **ğŸ“Œ Features & Modules**
### **ğŸ”¹ 1. Synthetic Dataset Generation**
| **Script** | `generate_fake_data_nextLevel.py` & `generate_fake_test_data.py` |
|------------|---------------------------------------------------------------|
| **Purpose** | Generates **synthetic linguistic datasets** with numerical linguistic indices. |
| **Features** | - Randomly assigns **classification labels**. <br> - Creates **linguistic index values** (Zipf Law, POS Distribution, Readability scores, etc.). <br> - Normalizes the dataset for **model training**. |
| **Output Files** | - `fake_data_nextLevel_normalized.csv` (Training set) <br> - `fake_data_test_normalized.csv` (Test set) |

### **ğŸ”¹ 2. Neural Network for Feature Dependencies**
| **Script** | `train_dependency_nn.py` |
|------------|--------------------------|
| **Purpose** | Trains a **Neural Network (NN)** to learn dependencies between linguistic features. |
| **Architecture** | - 3-layer fully connected NN. <br> - Uses **ReLU activations** and **Adam optimizer**. |
| **Feature Dependencies** | - Computes similarity-based penalties between features. <br> - Helps **regularize Maximum Entropy training**. |
| **Output Files** | `dependency_nn_best.pth` (Best trained model) |

### **ğŸ”¹ 3. Maximum Entropy Model Training**
| **Script** | `train_best_maxent_train_test_split.py` |
|------------|------------------------------------------|
| **Purpose** | Uses **Maximum Entropy principles** to estimate linguistic constraints. |
| **Key Concepts** | - **Lagrange multipliers** to enforce constraints. <br> - Supports **four constraints:** Mean, Variance, Skewness, Kurtosis. <br> - Optimized using **L-BFGS-B**. |
| **Neural Regularization** | - Uses the **pre-trained NN** to regularize constraints between features. |
| **Output Files** | `Lagrange_Multipliers.csv` |

### **ğŸ”¹ 4. Grid Search for Best Hyperparameters**
| **Script** | `maxent_grid_search.py` |
|------------|-------------------------|
| **Purpose** | Finds the **best regularization hyperparameters** for Maximum Entropy training. |
| **Hyperparameters** | - `Alpha` (L2 Regularization). <br> - `Beta` (Neural Network dependency penalty). |
| **Parallelization** | - Uses **multiprocessing** for fast grid search. |
| **Output Files** | `Grid_Search_Results.csv`, `Grid_Search_Loss_Heatmap.png` |

### **ğŸ”¹ 5. Statistical Significance Tests**
| **Script** | `statistical_analysis.py` |
|------------|----------------------------|
| **Purpose** | Tests **statistical significance** of constraints across linguistic levels. |
| **Tests Performed** | - **One-Way ANOVA** (Checks if constraint strengths differ across levels). <br> - **Kruskal-Wallis Test** (Non-parametric alternative). <br> - **Pairwise t-tests with Bonferroni correction**. <br> - **Effect size (Cohenâ€™s d)**. |
| **Output Files** | `Statistical_Analysis_Report.txt` |

### **ğŸ”¹ 6. Interpretation of Results**
| **Script** | `interpret_results.py` |
|------------|------------------------|
| **Purpose** | **Summarizes findings** from Maximum Entropy training. |
| **Key Insights** | - Which linguistic levels have **stronger constraints**? <br> - Does **feature hierarchy** hold (Character-Level > Morphology > Syntax)? |
| **Output Files** | `Interpretation_Report.txt` |

---

## **ğŸ“Š Visualizations**
- **Boxplot of Lagrange Multipliers** (`Lagrange_Multipliers_Boxplot.png`)
- **Heatmap of Feature Dependencies** (`NN_Feature_Dependencies_Heatmap.png`)
- **Effect Size Barplot** (`Effect_Size_CI_Barplot.png`)
- **Grid Search Heatmap** (`Grid_Search_Loss_Heatmap.png`)

---

## **ğŸ“Œ Installation**
```bash
# Clone the repository
git clone https://github.com/your-repo-name.git
cd Linguistic-Entropy-Model

# Install dependencies
pip install -r requirements.txt
```

---

## **ğŸ“Œ Usage**
### **1ï¸âƒ£ Generate Synthetic Data**
```bash
python generate_fake_data_nextLevel.py
python generate_fake_test_data.py
```
### **2ï¸âƒ£ Train Neural Network for Feature Dependencies**
```bash
python train_dependency_nn.py
```
### **3ï¸âƒ£ Train Maximum Entropy Model**
```bash
python train_best_maxent_train_test_split.py
```
### **4ï¸âƒ£ Run Hyperparameter Grid Search**
```bash
python maxent_grid_search.py
```
### **5ï¸âƒ£ Perform Statistical Analysis**
```bash
python statistical_analysis.py
```
### **6ï¸âƒ£ Generate Interpretation Report**
```bash
python interpret_results.py
```

---

## **ğŸ“œ License**
This project is licensed under the **MIT License**.

---

## **ğŸ‘¨â€ğŸ’» Author**
[Your Name]  
ğŸ”— [Your Website or GitHub Profile]  
ğŸ“© *For collaborations, feel free to reach out!* ğŸš€

